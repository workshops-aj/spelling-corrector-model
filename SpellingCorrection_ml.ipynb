{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73c382a-0162-4777-a44c-b64b54175008",
   "metadata": {},
   "source": [
    "# Spelling Correction using Seq2Seq LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486156d-d6b3-40c9-bad3-4bd693ea85c9",
   "metadata": {},
   "source": [
    "NOTE :  This dataset has been synthetically created for teaching and demonstration purposes. In real-world production use, spelling corrector models—especially sequence-to-sequence models with attention—should be trained on millions of diverse and domain-relevant examples to generalize well and provide robust performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b08e62-844b-4261-8b81-e8fee6c30f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49bffa-8139-4d9a-9947-0ec2b1c85340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88481f73-d337-4c46-bbe4-3321de0a5442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4703 - loss: 2.5991 - val_accuracy: 1.0000 - val_loss: 0.0771\n",
      "Epoch 2/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0472 - val_accuracy: 1.0000 - val_loss: 0.0130\n",
      "Epoch 3/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 0.0054\n",
      "Epoch 4/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
      "Epoch 5/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 6/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 7/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 8.8403e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.1620e-04 - val_accuracy: 1.0000 - val_loss: 6.4404e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.9895e-04 - val_accuracy: 1.0000 - val_loss: 4.8050e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.4823e-04 - val_accuracy: 1.0000 - val_loss: 3.6456e-04\n",
      "Input: she had to adress teh crowd with confidence.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Corrected: she had to address the crowd with confidence.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load synthetic spelling dataset\n",
    "df = pd.read_csv('synthetic_spelling_20k.csv')\n",
    "\n",
    "# Clean and lowercase\n",
    "input_texts = df['input'].astype(str).str.lower().tolist()\n",
    "target_texts = df['target'].astype(str).str.lower().tolist()\n",
    "target_texts = [f\"<start> {txt} <end>\" for txt in target_texts]\n",
    "\n",
    "# Train-test split\n",
    "train_input, val_input, train_target, val_target = train_test_split(input_texts, target_texts, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizers\n",
    "input_tokenizer = Tokenizer(oov_token='<unk>')\n",
    "input_tokenizer.fit_on_texts(train_input)\n",
    "input_seqs = input_tokenizer.texts_to_sequences(train_input)\n",
    "input_maxlen = max(len(seq) for seq in input_seqs)\n",
    "input_padded = pad_sequences(input_seqs, maxlen=input_maxlen, padding='post')\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "target_tokenizer = Tokenizer(oov_token='<unk>', filters='')\n",
    "target_tokenizer.fit_on_texts(train_target)\n",
    "target_seqs = target_tokenizer.texts_to_sequences(train_target)\n",
    "target_maxlen = max(len(seq) for seq in target_seqs)\n",
    "target_padded = pad_sequences(target_seqs, maxlen=target_maxlen, padding='post')\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Decoder input and output\n",
    "decoder_input_data = np.concatenate([np.zeros((len(target_padded), 1)), target_padded[:, :-1]], axis=1)\n",
    "decoder_output_data = to_categorical(target_padded, num_classes=target_vocab_size)\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 64\n",
    "lstm_units = 64\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(input_maxlen,))\n",
    "encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(target_maxlen,))\n",
    "decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention\n",
    "attention_layer = Attention()([decoder_outputs, encoder_outputs])\n",
    "concat_layer = Concatenate(axis=-1)([decoder_outputs, attention_layer])\n",
    "decoder_dense = Dense(target_vocab_size, activation='softmax')(concat_layer)\n",
    "\n",
    "# Compile model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_dense)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit([input_padded, decoder_input_data], decoder_output_data, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Inference function\n",
    "def correct_spelling(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    seq = input_tokenizer.texts_to_sequences([sentence])\n",
    "    pad_seq = pad_sequences(seq, maxlen=input_maxlen, padding='post')\n",
    "    decoder_seq = np.zeros((1, target_maxlen))\n",
    "    decoder_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
    "    result = \"\"\n",
    "    for i in range(1, target_maxlen):\n",
    "        preds = model.predict([pad_seq, decoder_seq])\n",
    "        pred_id = np.argmax(preds[0, i - 1, :])\n",
    "        pred_word = target_tokenizer.index_word.get(pred_id, '')\n",
    "        if pred_word == '<end>' or pred_word == '<unk>':\n",
    "            break\n",
    "        result += pred_word + ' '\n",
    "        decoder_seq[0, i] = pred_id\n",
    "    return result.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf12a80-6938-4290-a8af-e426b34c7e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e281a29f-3dc2-43b9-ac09-624185224cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: they need to acommodate all the invited guests.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Corrected: they need to accommodate all the invited guests.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example\n",
    "sample_input = \"they need to acommodate all the invited guests.\"\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Corrected:\", correct_spelling(sample_input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c1b57-5bce-4675-8ec7-561f7ef6a9b3",
   "metadata": {},
   "source": [
    "## NOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef64a8f-b920-4b0d-b16e-7ae71aca2495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
